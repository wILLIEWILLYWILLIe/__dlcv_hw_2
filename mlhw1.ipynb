{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wILLIEWILLYWILLIe/hw_1/blob/main/mlhw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      },
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWQh-lq8GuwZ",
        "outputId": "a6d99a11-851c-43f6-ce4e-58beacd6f0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_5Tf1rMHBQ-",
        "outputId": "c4f3dd43-b192-42ee-ca25-340184e91238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content\n",
            "drive  sample_data\n",
            "/content/drive/MyDrive/Colab_Notebooks/hw1\n",
            "/content/drive/MyDrive/Colab_Notebooks/hw1\n"
          ]
        }
      ],
      "source": [
        "from re import M\n",
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "%cd /content\n",
        "!pwd\n",
        "!ls\n",
        "%cd drive/MyDrive/Colab_Notebooks/hw1\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JywoPOO_Oll",
        "outputId": "b682d258-168d-4ca7-8e54-547d852610d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m289.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.12.2)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.4.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.0.3 googlesearch-python-1.3.0 lxml_html_clean-0.4.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX6SizAt_Olm",
        "outputId": "815c5511-e11b-4051-d10f-46fbf73bfef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScyW45N__Olm",
        "outputId": "3d1d603d-3943-4eba-f741-fb31abc83d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (32000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "token_max_len = 16384\n",
        "token_max_len = 32000\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=token_max_len,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]\n",
        "\n",
        "def check_len(myinput : str) -> str:\n",
        "    max_length = token_max_len\n",
        "    if len(myinput) >= max_length :\n",
        "      available_space = max_length - 4\n",
        "      myinput = myinput[:available_space] + \"...\"\n",
        "    return myinput\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dmGCARd_Oln",
        "outputId": "36b0b529-651b-4cbc-cb8c-7c01bd2c7036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n",
            "\n",
            "泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，她推出了多張專輯，包括《Taylor Swift》、《Fearless》，以及後來更為知名的大熱作品——「1989」。她以其寫作風格、歌曲創造力和表現能力而聞name。\n",
            "\n",
            "泰勒絲的音乐风味从乡村摇滚到流行搖擺，並且她的专辑《Fearless》、《Speak Now》，以及後來更為知名的大熱作品——「1989」、「Reputation」，都取得了巨大的成功。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}使用中文時只會使用繁體中文來回問題。\"},\n",
        "                # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages.\n",
        "                # A proper seperation text rather than a simple line break is recommended.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n===\\n{message}\"}\n",
        "            ]\n",
        "\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E4ygUhAZXou",
        "outputId": "e5a10bc4-d25f-4eef-ab81-a4838c93945e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "泰勒絲（Taylor Swift）是美國的一位知名流行音樂歌手、詞曲作家和製作人。她出生於1989年12月13日，肯塔基州曼徹斯特。她的父親是一個銀行經理，她的母校曾贏得全美最佳高中獎項。\n",
            "\n",
            "泰勒絲從小就對唱跳有濃厚興趣，並在2003年的農曆新歲時參加了「卡拉OK」比賽，獲得第一名。她之後開始寫作歌曲並錄製demo帶，以便向音樂公司推銷自己的作品。最終，她與大機器娛樂簽下唱片合約。\n",
            "\n",
            "泰勒絲的首張專輯《Taylor Swift》於2006年發行，並在美國獲得了商業上的成功。她之後陸續出版多部歌曲和专辑，包括「Fearless」、「Speak Now」，以及與斯考特·布倫南（Scott Borchetta）合作成立自己的唱片公司——泰勒絲共和國。\n",
            "\n",
            "她的音樂風格從鄉村搖滾逐漸轉變為流行電音，並且她以其對社會和個人問題的深刻描述，以及歌曲中的人物化手法而聞名。許多人認爺，她是當代最具影響力的女性藝術家之一。\n",
            "\n",
            "泰勒絲也是一位頗受關注的大眾人物，曾經與其他知星進行過公開爭議，並且她對社會和政治問題的表態備受到矚目。她在2020年發行了專輯《Folklore》，並於隔年的「1989世界巡迴演唱會」中宣布退休。\n"
          ]
        }
      ],
      "source": [
        "role_description = \"你是音樂與流行文化的專家，負責提供關於藝人與音樂的準確資訊。\"\n",
        "task_description = \"請根據使用者的問題，提供簡潔且準確的回答。\"\n",
        "task_description = \"\"\n",
        "agent = LLMAgent(role_description, task_description)\n",
        "\n",
        "# Test the question\n",
        "test_question = \"請問誰是 Taylor Swift？\"\n",
        "response = agent.inference(test_question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是問題解析專家，擅長從使用者的輸入中提取核心問題，並確保問題清晰且具體。\",\n",
        "    task_description=(\n",
        "        \"請分析以下輸入，提取核心問題\" #並包含問題重要條件，例如地點、人物、年代、區域。\\n\"\n",
        "        \"若輸入包含多個問題，請選擇最主要的一個。\\n\"\n",
        "        \"僅回答一句話即可。\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是關鍵詞萃取專家，負責從問題中提取最適合搜尋的關鍵詞，包括專有名詞、核心概念或動作及地點、\",\n",
        "    task_description=\"請從以下問題中提取關鍵詞，確保其具體且具代表性，用逗號分隔。避免過於泛泛的詞彙。\"\n",
        ")\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是問答生成專家，負責根據問題和相關資訊提供準確、簡潔且符合事實的回答。\",\n",
        "    task_description=(\n",
        "        \"請根據以下問題和篩選後的相關資訊，提供準確的回答。\\n\"\n",
        "        \"若有`相關資訊` 提供了明確的數字（如分數、金額、時間），請直接使用該數據來回答問題。\\n\"\n",
        "        \"請使用正式且易於理解的繁體中文回答，確保資訊清晰。\\n\"\n",
        "        \"一定要幫我整理好資料並輸出最好的繁體中文結果，這對我至官重要。\"\n",
        "    )\n",
        ")\n",
        "\n",
        "context_filter_agent = LLMAgent(\n",
        "    role_description=\"你是內容篩選專家，負責從搜尋結果中最相關的資訊，簡化內容以提高問答效率。\",\n",
        "    task_description=(\n",
        "        \"請根據以下問題，從相關資料中提取最正確的資訊，並用繁體中文回答問題。\\n\"\n",
        "        \"請確保答案與問題相關，優先保留數字資訊（如分數、金額、時間等）。\\n\"\n",
        "        \"若出現不同數字，請仔細比對何者為正確的。\\n\"\n",
        "        \"如果有72數字在內，擷取其數字。\\n\"\n",
        "        \"若有其他相關資訊或候選答案，也一併輸出。\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    DO_DEBUG = True\n",
        "    ########################################################\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens),\n",
        "    # You may want to truncate some excessive texts.\n",
        "    ########################################################\n",
        "    if DO_DEBUG : print(f'{\" \"*5}question: {question}')\n",
        "\n",
        "    # Step 1: Extract the core question\n",
        "    core_question = question_extraction_agent.inference(question)\n",
        "    if DO_DEBUG : print(f'{\" \"*5}core_question: {core_question}')\n",
        "    if not core_question:\n",
        "        return \"無法解析問題，請重新輸入。\"\n",
        "\n",
        "    # Step 2: Extract keywords from the core question\n",
        "    keywords = keyword_extraction_agent.inference(question)\n",
        "    if DO_DEBUG: print(f'{\" \"*5}key_words: {keywords}')\n",
        "    if not keywords:\n",
        "        return \"無法提取關鍵詞，請提供更具體的問題。\"\n",
        "\n",
        "    # # Step 3: Retrieve context using the provided search function\n",
        "    # raw_answer1 = qa_agent.inference(question)\n",
        "    # raw_search_results = await search(question, n_results=2)\n",
        "    # raw_search_results2 = await search(keywords, n_results=3)\n",
        "    # raw_search_results = [raw_answer1] + raw_search_results + raw_search_results2\n",
        "    # # if DO_DEBUG : print('   raw_result: ', raw_search_results)\n",
        "    # filter_output_tot = \"\"\n",
        "    # if raw_search_results :\n",
        "    #   for idx, raw_result in enumerate(raw_search_results):\n",
        "    #     filter_input  = check_len(f\"問題：{core_question}\\n相關資料: {' '.join(raw_result)}\")\n",
        "    #     filtered_context = context_filter_agent.inference(filter_input)\n",
        "    #     if not filtered_context: pass\n",
        "    #     else :\n",
        "    #       filter_output_tot += filtered_context\n",
        "    #       if DO_DEBUG :print(f'{\" \"*5}{idx}-filter_context\\n{filtered_context}')\n",
        "    # if not raw_search_results:\n",
        "    #     filter_output_tot = \"未找到相關資訊，根據已有知識回答。\"\n",
        "\n",
        "    # # Step 5: Generate the final answer\n",
        "    # final_input = f\"問題：{core_question}\\n相關資訊: {filter_output_tot}\"\n",
        "    # final_input = check_len(final_input)\n",
        "    # response = qa_agent.inference(final_input)\n",
        "\n",
        "    # check_len(response)\n",
        "\n",
        "    # return response\n",
        "\n",
        "    # Step 3: Retrieve context using the provided search function\n",
        "    raw_answer1 = qa_agent.inference(question)\n",
        "    raw_answer1 = \"\"\n",
        "    raw_search_results  = []\n",
        "    # raw_search_results += await search(core_question, n_results=1)\n",
        "    raw_search_results += await search(keywords, n_results=3)\n",
        "    for _ in raw_search_results :\n",
        "      raw_answer1 += _\n",
        "    final_input = check_len(f\"問題：{core_question}\\n相關資料: {raw_answer1}\")\n",
        "    response = qa_agent.inference(final_input)\n",
        "\n",
        "    check_len(response)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_kI_9EGB0S9"
      },
      "source": [
        "## Answer the questions using your pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN17sSZ8DUg7"
      },
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "plUDRTi_B39S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a1a9e4-9fd6-440e-ed01-a09e6c1f6606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     question: 李宏毅（Hung-yi Lee）是台灣著名的人工智慧與機器學習領域的學者，現任教於國立台灣大學電機工程學系。他是台灣大學的教授，也是該校人工智慧研究的領軍人物之一，尤其在機器學習、深度學習和語音處理領域具有深遠的影響。李宏毅教授在學術界的貢獻涵蓋了許多技術領域，特別是在語音處理、自然語言處理方面。他的課程，如「機器學習」和「深度學習」，在台灣乃至世界各地的學生中廣受歡迎，並且在網路上有許多免費的講座和教學資料，為學術界和業界培養了大量的人工智慧專業人才。李教授的研究興趣主要集中在深度學習的各種應用上，包括語音處理、視覺理解、生成模型等。他的學術成就不僅在學術論文上有所體現，還積極推動機器學習技術在實際生活中的應用。除了學術成就，李宏毅教授也是許多機器學習和人工智慧相關活動的活躍參與者，並致力於推動台灣在人工智慧領域的發展與創新。他的教學風格深入淺出，善於將複雜的理論問題以簡單明瞭的方式呈現，因此深受學生的喜愛。那李宏毅在台灣大學開設的《機器學習》 2023 年春季班中，第15個作業的名稱是什麼？\n",
            "\n",
            "     core_question: 李宏毅教授在《機器學習》2023年春季班中第15個作業的名稱是未知數，因此無法提供具體答案。\n",
            "     key_words: 李宏毅、人工智慧學者、中華民國大學教授機器学習深度学习\n",
            "========================================\n",
            "31 根据提供的信息，李宏毅教授在《机器学习》2023年春季班中第15个作业是关于生成式人工智能导论（Generative AI）。\n",
            "========================================\n",
            "     question: 目前臺灣多數獨立學院皆已升格為大學，公立的獨立學院僅剩一間，請問該獨立學院為何？\n",
            "\n",
            "     core_question: 該獨立學院是國防管理学院。\n",
            "     key_words: 獨立學院、公 立大學\n",
            "========================================\n",
            "32 根据提供的信息，独立学院是中国大陆现存的一种高等学校。它实施本科以上学历教育，是普通高校与国家机构以外社会组织或个人合作利用非财政性经费举办。  公有民辦二級學院（Independent College）是在1999年后出现于中國的獨立院校，主要是由一些原本屬於政府資助創建維持的大專學校轉型而來。這些學府在教育部2003年的《关于规范并加强普通高校以新的机制和模式试办独立学院管理》的通知中被正式確定為“獨立學院”。  公有民辦二級院校（Independent College）與其他類別的高等學校不同，它們主要是由政府資助創建維持的大專學府轉型而來。這些 學生在完成学业后，由山东大学颁发统一 的毕业证书。  独立学院与公立高校、民办学校有所区别，特别是在招生的方式和录取分数线上都存在差异。在中国大陆的教育系统中，一般来说，“校區”是指與“本部(主 校 區)”除地理位置不同外其余方面皆保持一致 的 “ 本 學 院 分 部 ”，而不是独立学院。\n",
            "========================================\n",
            "     question: BitTorrent 協議曾經在臺灣掀起一陣盜版影音的歪風，該協議運用何種機制來確保當一個新的節點加入網路，尚無任何 chunk 時，也能從其他種子隨機地獲得部分資料，以利其後續整個網路的資料交換？\n",
            "\n",
            "     core_question: 核心問題是：BitTorrent 協議如何確保新節點能夠從其他種子隨機獲得部分資料？\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "BitTorrent 協議、節點加入網路、新 chunk 取得機制\n",
            "========================================\n",
            "33 BitTorrent 協議如何確保新節點能夠從其他種子隨機獲得部分資料？  答： Bit Torrent 的協定使用了一個稱為「分片」的技術，將大型案切割成小塊（通常是 16KB 或更大的大小），並且每一份都有多餘的複製。這樣做可以讓新節點從其他種子隨機獲得部分資料，並組合起來形成完整的大文件。  在 Bit Torrent 中，當一個用戶下載檔案時，他會與網路上的各個分享者建立連線，以取得所需的小塊。如果某一份小片段的複製數量足夠多，那麼新節點就可以從其中任何的一位種子獲得該部分資料。這樣做不僅能提高下載速度，也使得檔案分佈更加均衡。  此外，Bit Torrent 的協定還使用了一個稱為「哈希值」的技術，用於識別每一份小片段的唯一性。如果某位分享者提供了相同的小塊，那麼新節點就可以確認它是正確無誤，並且不需要再次下載。這樣做能夠提高檔案完整性的安全度。  總之，Bit Torrent 的協定通過分割大型文件、使用多餘的複製和哈希值等技術來實現新節點從其他種子隨機獲得部分資料，並組合起形成整個的大大小小。\n",
            "========================================\n",
            "     question: 前幾天聽朋友在滑youtube的影片，說看到一個人用英文跟另一個騎著摩托車的人說要去某某大學能不能載他一程，騎摩托車的人馬上說一聲sure答應了請求，並說甚麼go back go back要他上車，結果出發後沒幾秒就摔車了。那個是甚麼影片阿？\n",
            "\n",
            "     core_question: 核心問題是：你想知道那個youtube影片的內容或名稱。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "滑動門、摩托車人士、一起出行、高速公路\n",
            "========================================\n",
            "34 根據相關資訊，我發現了你想要知道的內容是：重慶官方指當地摩托車9月起可上高速公路傳言不實。  這個消息源自於《新明日报早報》的一篇文章，該文提到了一則網路流传謠 言，即將在 2024 年九 月一 日開始允許重慶地區的汽摩（含電動車）上高速公路。然而，這項資訊被證實為不真確。  根據《道路交通安全法》和相關規定，非機动车、包括电瓶三轮汽车等，不得在城市快速路或高架桥行驶，也不得进入收费站；摩托車也不能上高速公路。\n",
            "========================================\n",
            "     question: 根據許多來自各方的觀察報告以及相關研究資料顯示，研究人員在奧地利維也納大學獸醫學院中，十分驚奇地且仔細地發現，戈芬氏鳳頭鸚鵡（亦稱為Goffin's cockatoo）竟然會將食物浸入乳酪之中，以期能夠提升口感與風味。值得一提的是，這項相當具有啟發性與創新性的研究成果，已經正式發表於廣受國際科學界重視的《當代生物學》（Current Biology）期刊，也因此引起了許多專家學者與科學家們的高度關注與進一步討論。根據他們在實驗當中所進行的繁複且嚴謹的觀察記錄，發現這些鳳頭鸚鵡似乎喜愛將經過煮熟處理的馬鈴薯塊，先行浸入特定的乳酪醬料之後再行食用，而這樣的動作似乎暗示它們認為經此步驟能夠讓食物變得更加美味。進一步且更深入的實驗與分析結果也顯示，這些戈芬氏鳳頭鸚鵡不僅會在不同情況下嘗試各式各樣的食物與沾醬搭配，而且尤其顯示出它們對某一特定口味或風味的乳酪抱持著特別顯著的偏好；這一點充分證明了它們這種將食物浸入乳酪的行為，不僅僅只是為了單純增加食物濕潤度或方便食用，而更是展現了它們透過浸泡與翻轉的步驟，來強化與提升整體的口味與風味特色。此外，在實驗過程中，也可以清楚觀察到，這些鸚鵡在進行沾醬的同時，會特意反覆翻轉並輕輕地壓製食物，好讓乳酪醬料能夠更加均勻且深入地覆蓋到食材表面，而並非僅僅只是以舌頭直接舔食那個乳酪醬而已。從中我們也看出它們對食材與醬料之間的配合度，以及口感層次的提升，具有一定程度的講究與偏好。綜合上述研究結果，包含該項研究在《當代生物學》期刊的發表內容，以及科學界對此行為模式所展現的興趣，我們可以得知這些戈芬氏鳳頭鸚鵡不僅擁有在食物處理與風味提升上的靈活性，更展現了一種令人驚艷的挑食與口味鑑別能力。因此，在這些看似豐富而又極具趣味性的發現背後，我們也不禁要進一步提出疑問：既然它們會特別挑選某種口味的乳酪來提升風味，那麼究竟在這些眾多多元且可能性豐富的乳酪口味當中，最終令戈芬氏鳳頭鸚鵡展現出顯著偏好的口味是哪一種呢？換句話說，請問在這整個實驗結果中，最受這些鸚鵡偏好且深具吸引力的乳酪口味到底是什麼呢？\n",
            "\n",
            "     core_question: 核心問題是：戈芬氏鳳頭鸚鵡最喜歡哪種乳酪口味？\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "戈芬氏鳳頭鸚鶴、奧地利維也納大學獸醫學院、《當代生物學》期刊、中性乳酪醬料、マシュマロ奶油或其他特殊口味\n",
            "========================================\n",
            "35 很抱歉，但我無法找到任何關於戈芬氏鳳頭鸚鵡喜歡的乳酪口味相關資訊。\n",
            "========================================\n",
            "     question: 2024年，住在桃園Xpark水族館的國王企鵝「嘟胖」以及「烏龍茶」順利產下一隻企鵝寶寶，Xpark也因此於網路上舉行了企鵝命名的投稿和名稱投票活動，最後這隻企鵝寶寶的名子是？\n",
            "\n",
            "     core_question: 核心問題：2024年Xpark水族館的企鵝寶貝叫什麼名字？\n",
            "     key_words: 桃園、Xpark水族館、「嘟胖」和「烏龍茶」，企鵝寶嬰\n",
            "========================================\n",
            "36 根據提供的資訊，桃園Xpark水族館中的國王企鵝寶貝叫「烏龍茶」與台北市立動物园国民小天使嘟胖生的新仔。\n",
            "========================================\n",
            "     question: 現代人開始注重運動，也因此運動傷害的問題逐漸受到重視，許多人遇到問題時會尋求物理治療師的協助。目前國立臺灣大學物理治療學系的正常修業年限為幾年？\n",
            "\n",
            "     core_question: 核心問題是：國立臺灣大學物理治療學系的正常修業年限為幾個月？\n",
            "     key_words: 物理治療師、國立臺灣大學\n",
            "========================================\n",
            "37 根據提供的資訊，國立臺灣大學物理治療學系正常修業年限為六個月。\n",
            "========================================\n",
            "     question: 「呼嘿嘿」是《BanG Dream!》中哪位角色的笑聲習慣？\n",
            "\n",
            "     core_question: 提取核心問題：《BanG Dream!》中哪位角色笑聲習慣是「呼嘿 嘻」？\n",
            "     key_words: 《BanG Dream!》, 角色\n",
            "========================================\n",
            "38 根据提供的信息，笑声习惯是“呼嘿 嘻”的角色为市谷有咲（日语：いちがやありさ）。\n",
            "========================================\n",
            "     question: 模壽是中國知名的模型廠商，其旗下的原創漫畫《先祖效應》中出現紅色外觀加上日本武士風格的機體，模壽將之命名為「甲斐之虎」。日本戰國時代被稱為「甲斐之虎」的人物是誰？\n",
            "\n",
            "     core_question: 核心問題是：日本戰國時代被稱為「甲斐之虎」的人物誰？\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "甲斐之虎、模壽、《先祖效應》\n",
            "========================================\n",
            "39 根據相關資訊，我們可以得知日本戰國時代被稱為「甲斐之虎」的人物是:  **武田信玄**  他是一位著名的軍事領袖和政治家，活躍於16世紀末至17世纪初期。他的統治使德川幕府時候前後都對其進行了多次戰爭。  希望這個答案能夠幫助你！\n",
            "========================================\n",
            "     question: 終於到了台灣大學114年度的選課時期，同學們總會在朋友圈流傳各種意義上的好課，可能是「你可以不修，但你一定要請你朋友修」的好課，也可能是「值得一修再修」的好課。剛加入台大的王肥貓同學，正在為選擇通識課而煩惱，想要在他的候選名單選出網路上最多好評的課程。王肥貓的候選名單中有「國民法官必備之基礎鑑識科學」、「現代中國與世界：1911-1979」以及「數位素養導航」。請問依照王肥貓同學的標準，他最有可能去修哪一門課？\n",
            "\n",
            "     core_question: 王肥貓同學最有可能去修的課程是「數位素養導航」，因為它在網路上獲得了最高評價。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "台大、選課時期、高評價通識课、中華民國法律學科基礎鑑證法/數位素養導航\n",
            "========================================\n",
            "40 根據提供的資訊，我無法找到任何關於王肥貓同學或「數位素養導航」課程相關信息。\n",
            "========================================\n",
            "     question: 《極限體能王SASUKE》是日本TBS電視台不定期播出的運動娛樂特別節目，其身受日本觀眾喜愛，在日本擁有高收視率。《極限體能王SASUKE》在全世界有不小的知名度，並不僅僅是在不同國家播出節目，甚至在18個國家或地區還與當地電視台合作製作在地版的《極限體能王SASUKE》節目。請問2024年的第42回《極限體能王SASUKE》在2024年的哪一天首播？\n",
            "\n",
            "     core_question: 無法提供2024年的第42回《極限體能王SASUKE》首播日期，因為這個資訊尚未公開。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "極限體能王SASUKE、TBS電視台、日本觀眾、高收視率、《第42回》\n",
            "========================================\n",
            "41 很抱歉，但無法提供2024年的第42回《極限體能王SASUKE》首播日期，因為這個資訊尚未公開。\n",
            "========================================\n",
            "     question: 許多人對原住民族有所誤解，認為原住民部落自古都很排外，然實際上原住民族與外族的接觸很早便有了，甚至也曾出現過漢人擔任部落頭目的故事。請問出身於利嘉部落，後來成為初鹿部落頭目的漢人，名為？\n",
            "\n",
            "     core_question: 根據史料，出身於利嘉部落後來成為初鹿頭目的漢人是張添財。\n",
            "     key_words: 利嘉部落、初鹿 department 頭目\n",
            "========================================\n",
            "42 根據史料，出身於利嘉部落後來成為初鹿頭目的漢人是張添財。\n",
            "========================================\n",
            "     question: 《BanG Dream!》是連載於《月刊武士道》的漫畫作品，後來成為了跨媒體發行的大型企劃。《BanG Dream! Ave Mujica》的片頭曲是哪一首？\n",
            "\n",
            "     core_question: 核心問題是：《BanG Dream! Ave Mujica》的片頭曲是什么？\n",
            "     key_words: BanG Dream!、月刊武士道\n",
            "========================================\n",
            "43 《BanG Dream! Ave Mujica》的片头曲是「迷星叫」（日语：めいせいかけ）。\n",
            "========================================\n",
            "     question: Linux作業系統最早於哪一年首次發布？\n",
            "\n",
            "     core_question: 核心問題是：Linux作業系統最早於哪一年首次發布？\n",
            "     key_words: Linux作業系統、首次發布年份\n",
            "========================================\n",
            "44 Linux作業系統最早於1991年首次發布。\n",
            "========================================\n",
            "     question: 負責本次作業的助教是來自臺東的卑南族，來自 Likavung 部落，是一個在臺東縣卑南鄉的靜謐之地，近期 Likavung 部落積極復振傳統文化，並於前年興建傳統男子會所「巴拉冠」。請問 Likavung 的中文名稱為何？\n",
            "\n",
            "     core_question: 核心問題是：Likavung 的中文名稱為何？\n",
            "     key_words: 卑南族、Likavung 部落\n",
            "========================================\n",
            "45 根據提供的資訊，利嘉部落（Likavung）的中文名稱為「」或譯作\"Rik'avon”。\n",
            "========================================\n",
            "     question: 紅茶是全發酵茶類還是不發酵茶類？\n",
            "\n",
            "     core_question: 紅茶屬於不發酵的黑 trà類。\n",
            "     key_words: 紅茶、全發酵、中性酸度\n",
            "========================================\n",
            "46 红茶通常被认为是微酸性的。虽然其pH值会因不同的冲泡方式和tea叶种类而有所变化，但总体上，red tea的acid度相对较低接近中性。  饮用时, red Tea中的成分将在身体产生轻 微 的 acid 性反应但不会显著影响 body 内部酸碱平衡。\n",
            "========================================\n",
            "     question: 在《遊戲王》卡牌遊戲中，以「真紅眼黑龍」與「黑魔導」作為融合素材的融合怪獸是什麼？\n",
            "\n",
            "     core_question: 核心問題是：《遊戲王》中，以「真紅眼黑龍」與 「 黑魔導 」作為融合素材的哪一隻怪獸？\n",
            "     key_words: 真紅眼黑龍、 黑魔導\n",
            "========================================\n",
            "47 根據問題和相關資訊，我們可以得出以下結論：  《遊戲王》中，以「真紅眼黑龍」與 「 黑魔導 」作為融合素材的怪獸是：超級魔法騎士- 真红眼睛骑兵\n",
            "========================================\n",
            "     question: 豐田萌繪在《BanG Dream!》企劃中，擔任哪個角色的聲優？\n",
            "\n",
            "     core_question: 豐田萌繪在《BanG Dream!》中擔任Rita角色聲優。\n",
            "     key_words: 豐田萌繪、BanG Dream!\n",
            "========================================\n",
            "48 豐田萌繪在《BanG Dream!》中擔任Rita角色聲優。\n",
            "========================================\n",
            "     question: Rugby Union 中，9 號球員的正式名稱為何？\n",
            "\n",
            "     core_question: 核心問題：Rugby Union 中，9 號球員的正式名稱為何？\n",
            "     key_words: 提取的關鍵詞：Rugby Union、9 號球員\n",
            "========================================\n",
            "49 根據提供的資訊，9 號球員在 Rugby Union 中被稱為「Scrum-half」。  這個答案是基於相關資料中對 Sc rum- half 的描述而得出的。\n",
            "========================================\n",
            "     question: 曾被視為太陽系中的行星，最終被降格成矮行星的星球為何？\n",
            "\n",
            "     core_question: 核心問題是：哪顆星球被降格成矮行 星？\n",
            "     key_words: 提取的關鍵詞：太陽系、行星矮행 星球\n",
            "========================================\n",
            "50 根据提供的信息，太阳系中被降格成矮行星的是冥王 星（Pluto）。\n",
            "========================================\n",
            "     question: 以往政府對動物保護的觀念僅停留在寵物，因此動保法又被調侃為可愛動物保護法，近年來政策逐漸重視野生動物的保護。臺灣最早成立的野生動物救傷單位位於哪個行政區內？\n",
            "\n",
            "     core_question: 核心問題是：臺灣最早成立的野生動物救傷單位位於哪個行政區內？\n",
            "     key_words: 動物保護、野生动物保护法、新北市\n",
            "========================================\n",
            "51 根據提供的資訊，臺灣最早成立野生動物救傷單位位於新北市。\n",
            "========================================\n",
            "     question: 位於南投縣集集鎮的特生中心是親子育樂的好去處，館內以臺灣本土生態及生物為主軸，規劃高、中、低海拔生態系、特有動物、特有植物、環境-生物-人、自然保育、植物的奧秘及動物的奇觀等主題展區。特生中心在2023年改名，目前該單位的名字為？\n",
            "\n",
            "     core_question: 核心問題是：特生中心在2023年改名後的名字為何？\n",
            "     key_words: 南投縣、集集中特生中心改名\n",
            "========================================\n",
            "52 根據提供的資訊，特生中心在2023年改名後被稱為「農業部生物多樣性研究所」。\n",
            "========================================\n",
            "     question: Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data論文中提出的模型是甚麼名字？\n",
            "\n",
            "     core_question: 該模型的名字是\"Instruction-Tuning-Free Instruction-Following Speech Language Model（ITF-IFSLM）”。\n",
            "     key_words: instruction-following、speech language model\n",
            "========================================\n",
            "53 根據提供的資訊，這個模型被稱為\"Instruction-Tuning-Free Instruction-Following Speech Language Model（ITF-IFSLM）”。\n",
            "========================================\n",
            "     question: 請問太陽系中體積最大的行星是哪一顆？\n",
            "\n",
            "     core_question: 太陽系中體積最大的行星是土壤。\n",
            "     key_words: 太陽系、體積最大的行星\n",
            "========================================\n",
            "54 太陽系中體積最大的行星是木衛三的直徑大於地球，然而根據相關資訊顯示：\" 木卫一、2号和3號都是伽利略发现的一组巨型天然气体环绕着Jupiter的大球形月亮。其中最大的是(木)...\"\n",
            "========================================\n",
            "     question: 在語言分類學上，臺灣目前法定的十六個原住民族語言皆屬於南島語系，然其中一族的語言與其他語言在分類學上一般不被視為同一群，請問是哪一族的語言與其他語言親緣關係最遙遠？\n",
            "\n",
            "     core_question: 最主要的核心問題是：哪一族原住民族語言與其他南島系语言親緣關係最大？\n",
            "     key_words: 南島語系、原住民族, 臺灣\n",
            "========================================\n",
            "55 根據相關資訊，台灣南島語言與其他 南岛语系语言親緣關係最大的是泰雅群、西北組（包括龜崙族）、鄒 語 群 、魯凱 類 （包含霧臺東萬山多納茂林）排灣 組 卑農 细分為南王知本建和布农组等。\n",
            "========================================\n",
            "     question: 相傳在台灣大學的某一堂程式設計課程從來不會進行分組，如果有人問為甚麼沒有分組，老師總會說「我上課都不分組的。分組的結果通常都是一個阿宅在寫code；一個未來可以當PM的在台上吹牛；一個廢柴跑去買便當」。請問講出這句話的老師是誰？\n",
            "\n",
            "     core_question: 這個問題的核心是找出講出的那句話老師的人物身份。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "台灣大學、程式設計課 程序老師\n",
            "========================================\n",
            "56 根據相關資訊，判決字號為112年憲斷第11 號。\n",
            "========================================\n",
            "     question: 臺灣原住民族的語言非常多元，有各式各樣的打招呼用語，比方說布農族的「uninang mihumisang」、阿美族的「nga'ay ho」。「embiyax namu kana」 是哪一臺灣原住民族的打招呼用語？\n",
            "\n",
            "     core_question: 核心問題是：「embiyax namu kana」 是哪一臺灣原住民族的打招呼用語？\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "布農族、阿美족語言原住民族打招呼用字\n",
            "========================================\n",
            "57 根據提供的資訊，「embiyax namu kana」 是布農語的一個打招呼用詞。\n",
            "========================================\n",
            "     question: 鄒族與布農族生活區域大量重疊，最開始鄒族因為驍勇善戰，因此擁有大量土地，後來因為外族人帶來的瘟疫，使得鄒族的族群勢力快速下滑，並與布農族人混居。請問「鄒與布農，永久美麗」這句話與哪個鄒族、布農族混居的部落息息相關？\n",
            "\n",
            "     core_question: 這句話與「阿里山」息 息相關。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "鄒族、布農 tribe 、混居部落\n",
            "========================================\n",
            "58 根據提供的資訊，我們無法直接找到與「阿里山」相關聯的事實或數字。然而，基於一般知識，可以推測你可能在問的是台湾南投縣的一個著名觀光景點——\" 阿 里 山 \"。  如果是這樣，那麼我可以告訴您一些基本資訊：  *   位處：台灣的中部山區     * 位置: 台灣省 南 投 縣  阿里山市是一座位於台湾南投县的一個著名觀光景點，知 名於其優美風 景、豐富生態和多樣化旅遊活動。\n",
            "========================================\n",
            "     question: 動畫「雖然是公會的櫃檯小姐，但因為不想加班所以打算獨自討伐迷宮頭目」中女主角隱藏的冒險者身份是甚麼？\n",
            "\n",
            "     core_question: 核心問題是：動畫中女主角隱藏的冒險者身份是什么？\n",
            "     key_words: 動畫、女主角身份\n",
            "========================================\n",
            "59 根据问题和相关信息，答案是：  在动画中女主角隱藏的冒險者身份是什么？  答：具体到哪一部作品我需要更多详细的问题来回答。\n",
            "========================================\n",
            "     question: 在卑南族的傳說中，有一對姊弟（Tuku 及 Sihasihau），這對姊弟後來各自創建了一個部落，其中姊姊 Tuku 後來創建了哪一個部落？\n",
            "\n",
            "     core_question: 核心問題是：姊妹Tuku創建了哪一個部落？\n",
            "     key_words: Tuku、卑南族\n",
            "========================================\n",
            "60 根據相關資訊，姊妹Tuku創建了射馬干部落。\n",
            "========================================\n",
            "     question: 2005 播出的電視劇《終極一班》中，有一個高中生戰力排行榜，稱為「KO榜」，該榜榜首為？\n",
            "\n",
            "     core_question: 核心問題是：《終極一班》中的「KO榜」第一名為誰？\n",
            "     key_words: 戰力排行榜、KO棒\n",
            "========================================\n",
            "61 根据维基百科的信息，终极一班3中的KO榜第一名是中万钧（SpeXial-子閎）。\n",
            "========================================\n",
            "     question: Linux kernel 曾使用的 process scheduler --- completely fair scheduler (CFS) 採用了何種資料結構來儲存排程相關資訊？\n",
            "\n",
            "     core_question: 核心問題是：Linux kernel 的完全公平排程器 (CFS) 使用何種資料結構儲存 排 程 相關資訊。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "Linux kernel、completely fair scheduler (CFS)、資料結構\n",
            "========================================\n",
            "62 Linux kernel 的完全公平排程器 (CFS) 使用红黑树数据结构来储存进 程相关信息。\n",
            "========================================\n",
            "     question: 請問第二次世界大戰中，盟軍在歐洲發動的最大規模登陸作戰諾曼第登陸（Normandy Landings），其作戰代號為何？\n",
            "\n",
            "     core_question: 盟軍在諾曼第登陸的作戰代號是「奧運會」（Operation Overlord）。\n",
            "     key_words: 登陸作戰、諾曼第、大西洋壁壘破裂行動（Operation Overlord）、盟軍\n",
            "========================================\n",
            "63 盟军在诺曼底登陆的作战代号是“奥运会”（Operation Overlord）。\n",
            "========================================\n",
            "     question: 《Cytus II》遊戲中「Body Talk」是哪位角色的歌曲？\n",
            "\n",
            "     core_question: 核心問題是：《Cytus II》遊戲中「Body Talk」歌曲屬於哪位角色？\n",
            "     key_words: 提取的關鍵詞：《Cytus II》、Body Talk\n",
            "========================================\n",
            "64 很抱歉，但根據提供的資訊，我無法直接找到《Cytus II》遊戲中「Body Talk」歌曲屬於哪位角色相關資料。\n",
            "========================================\n",
            "     question: 李琳山教授 1974 年畢業於國立臺灣大學電機工程學系，並且在 1975 年及 1977 年由美國史丹佛大學取得電機工程碩士及博士學位。他在國立臺灣大學所開設的信號與系統課程，在期末考前後會有一次演講，該演講又被稱為？\n",
            "\n",
            "     core_question: 李琳山教授的演講被稱為「信號與系統課程」的期末考前後一次特別評估，該名詞是 \"Midterm\" 的中文譯本。\n",
            "     key_words: 李琳山教授、國立臺灣大學電機工程學系、中美交流(或美國史丹佛大学)、信號與系統課程\n",
            "========================================\n",
            "65 根據提供的資訊，我們可以得出以下結論：  李琳山教授是一位電機工程學系教師，他在台大任職多年，研究領域包括人造衛星通讯等。他的演講被稱為「信號與系統課程」，該名詞是 \"Midterm\" 的中文譯本。  因此，我們可以確定李琳山教授的 Mid-term 考試是一個重要的一部分，他在台大電機系任教時，曾經開過一門叫做 « 信号与系统 »（Signal and System）的課程。\n",
            "========================================\n",
            "     question: 唉，我朋友總是一直說他很窮，買不起輝達最新的 5090 顯卡。他還一直強調 5090 顯卡不只是在算力方面超過過去的 4090，記憶體量也有關鍵性的提升，讓他可以部屬更大的 LLM。那 RTX 5090 的 VRAM 到底是多大？\n",
            "\n",
            "     core_question: 核心問題是：RTX 5090 顯卡的 VRAM 是多少？\n",
            "     key_words: 顯卡、5090 顯 卡 、409  RTX系列\n",
            "========================================\n",
            "66 根據提供的資訊，RTX 5090 顯卡擁有32 GB 的 VRAM。\n",
            "========================================\n",
            "     question: 棒球一直是風靡全台灣的運動之一，台灣棒球歷史也相當的悠久，自日治時期的台灣開始便有棒球運動的紀錄。而中華職棒作為台灣目前唯一的職業棒球聯盟，更是台灣棒球的重要象徵之一。台灣的棒球也享譽國際，是各大棒球國際賽的常客之一。請問在2024年所舉辦的「世界棒球12強賽」中，冠軍為哪一隊？\n",
            "\n",
            "     core_question: 提取核心問題：2024年世界棒球12強賽的冠軍是哪一隊？\n",
            "     key_words: 棒球、台灣、中華職業聯盟世界12強賽\n",
            "========================================\n",
            "67 根據相關資訊，2024年世界棒球12強賽的冠軍是中華臺北。\n",
            "========================================\n",
            "     question: 中國四大奇書是哪幾本？\n",
            "\n",
            "     core_question: 核心問題是：中國四大奇書的內容。\n",
            "     key_words: 四大奇書、中國古典文學\n",
            "========================================\n",
            "68 根據相關資訊，我們可以得出以下結論：  中國四大奇書是指《水滸傳》、《三國演義》，以及兩部神魔小說：金瓶梅和西遊記。這些作品代表了中古時期的文學風格，具有濃郁的人物形象、豐富的情節，以及對社會現實進行諷刺批評。  四大奇書中的《水滸傳》是中國歷史上第一部描寫農民起義的小說，全书圍繞“官逼민反”这一线索展开情节，表现了一群不堪暴政欺压的好汉揭杆而 起、聚义梁山直至接受招安致使叛亂失败。  《三國演義》則是以蜀漢為中心，以 三国矯盾斗争为主线来组织全书的情節，展現了公元184年到280年的历史风云画卷。它成功地塑造了一众鲜明的人物形象，並長于描述战争。  金瓶梅和西遊記則是兩部神魔小說，它們以豐富奇特的艺术想像、生動曲折的情節，栩 栗如生的人物 形 象，以及幽默诙谑 的 語言 等等 特徵。\n",
            "========================================\n",
            "     question: 中國時辰中的子時，如果用24小時制表示，是幾點到幾點？\n",
            "\n",
            "     core_question: 核心問題是：子時在24小时制中的時間範圍。\n",
            "     key_words: 子時、24小時間段\n",
            "========================================\n",
            "69 根据提供的資訊，子時在24小时制中的時間範圍是23點至1点。\n",
            "========================================\n",
            "     question: 一般學生總是有許多課堂作業需要完成，通常學生們會順著作業的死線將時序拉近，因為遲交作業可能讓作業不算分或是作業分數打折。而電腦也不例外，電腦有著排程演算法來決定接下來要執行什麼動作。請問在作業系統中，避免要錯過時限來完成作業的排程演算法稱為什麼？\n",
            "\n",
            "     core_question: 核心問題是：在作業系統中，避免要錯過時限來完成任務的排程演算法稱為「即期式」或 Real-Time Scheduling。\n",
            "     key_words: 排程演算法、時限\n",
            "========================================\n",
            "70 根據提供的資訊，核心問題是：在作業系統中，用於避免錯過時限來完成任務排程演算法稱為「即期式」或 Real-Time Scheduling。  最快需要花多久時間？─工業工程實際應用  答案如下：  根據提供的資訊，最佳解是先將兩支手機充電各半個小時，然后拿開一隻暫不續傳、另一邊繼続传。等到其中一個完成後，再把另外那條還沒完全補給的手提電話取回來重新開始。  因此，最快需要花費的時間是1.5 小时（即 90 分钟）。  最少平均遺漏排程演算法  根據提供資訊，具有時限讯務 最小遗失 排序 演 算 法 (ALeastMeanLossAlgorithmforSchedulingTrafficwithDeadlines) 是一個基於 LMPL 的策略，用來避免錯過時間限制的任意排序。  這個方法計算封包遺漏數據是根 據使用者佇列最前面的時限及目前通道失誤率。它導出了一種遞迴演算法，實例中顯示此策略可獲得比 EDF 或 FEDF 策 略更好的性能。  因此，最快需要花費的時間是 1.5 小时（即90 分钟）。\n",
            "========================================\n",
            "     question: 在2010年代初期由日本輕小說改編的知名動畫系列《刀劍神域》中，存在一組以英文字母「C」開頭搭配四位數「8763」構成的招式代號，此招式因施展時會出現十六道連續斬擊軌跡與金色光效而廣為觀眾所知。請具體回答：該代號「C8763」在原作中明確對應哪位角色持有的劍技？\n",
            "\n",
            "     core_question: 該代號「C8763」在原作中明確對應於黑乃雪花（Suguha Kirigaya）持有的劍技。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "刀劍神域、C8763\n",
            "========================================\n",
            "71 對不起，我們無法找到相關資訊。\n",
            "========================================\n",
            "     question: 曾經風靡一時的影集《斯卡羅》描述的是斯卡羅族的故事，劇中之地名「柴城」位於現今的哪個行政區劃？\n",
            "\n",
            "     core_question: 核心問題是：劇中之地名「柴城」位於現今的哪個行政區劃？\n",
            "     key_words: 柴城、斯卡羅族\n",
            "========================================\n",
            "72 根據相關資訊，劇中之地名「柴城」位於現今的屏東縣車 城（統埔村）。\n",
            "========================================\n",
            "     question: Google Colab的訂閱制中，若要使用A100高級GPU，需要訂閱「Colab Pro」還是「Colab Pro+」？\n",
            "\n",
            "     core_question: 核心問題是：Google Colab的訂閱制中，A100高級GPU需要哪個層次「Colabor Pro」或 「CollaboPro+」。\n",
            "     key_words: Google Colab、A100、高級GPU、中 Google 訂閱制\n",
            "========================================\n",
            "73 根據提供的資訊，我們可以得出以下結論：  Google Colab Pro+訂閱制中，A100 高級 GPU 需要「CollaboPro +」層次。  這是因為在 Google Collbo 的計算單元限制下，每個 A 1,000 處理器 (GPU) 每小時消耗的计算单位數量比 T4-GPU 更高。根據提供資訊，T-100 GPU 需要每小时13.08个单位，而 Colab Pro+層次才可以使用這種強大的計算單元。  因此，如果您想要在 Google Collbo 中利用 A 1,000 處理器 (GPU)，則需要購買「CollaboPro +」的訂閱制。\n",
            "========================================\n",
            "     question: 台灣大學中由李宏毅老師開設的機器學習，是屬於哪個學院的課程？\n",
            "\n",
            "     core_question: 核心問題是：李宏毅老師開設的機器學習課程屬於哪個學院？\n",
            "     key_words: 李宏毅、台灣大學、中山學系電機工程研究所\n",
            "========================================\n",
            "74 根據提供的資訊，李宏毅老師是國立台灣大學電機工程學系教授，也有與計算机科學院合作。\n",
            "========================================\n",
            "     question: 就讀國立臺灣大學資工系的雪江同學，正在為 113 學年度第 2 學期選課的事情煩惱著，身為一位大三學生，他正在考慮兩個修課的策略。第一個是多修一些課湊到不用低修的學分數爭取獲得書卷獎的機會；第二個是大笑三聲，減修學分申請書直接簽下去，專心在專題研究上。如果雪江同學選擇第一個策略，它至少要修多少學分才可以不用簽減修學分申請書？\n",
            "\n",
            "     core_question: 雪江同學至少需要修滿 120 學分，才能不用簽減免申請書。\n",
            "     key_words: 就讀、國立臺灣大學資工系、大三學生、三年級修課策略書卷獎減免申請\n",
            "========================================\n",
            "75 根據提供的相關資訊，我們可以得出以下結論：  雪江同學至少需要修滿 120 學分，才能不用簽減免申請書。  然而，這個問題與國立臺灣大學財務或獎勵政策無關。因此，並沒有直接提及「一定要幫我整理好資料並輸出最好的繁體中文結果」的相關資訊。但是，我們可以根據提供的學生成績優良奖励同分參酌順序，要點進行分析。  在這個文件中，國立臺灣大學財務或獎勵政策沒有直接提及「修滿 120 學 分」之要求。然而，這可能與其他相關資訊有關，如課程需求、學位規定等。但是，並無法確切得知雪江同儕需要多少分數。  因此，我們只能根據提供的文件進行分析，沒有直接答案可以給出。如果您想知道更多信息或具體要求，可以試著聯繫國立臺灣大學資訊工程學系。\n",
            "========================================\n",
            "     question: 知名 AI VTuber「Neuro-sama」最初的 Live2D 模型是使用 VTube Studio 的哪個角色？\n",
            "\n",
            "     core_question: Neuro-sama 的 Live2D 模型最初是使用 VTube Studio 預設角色。\n",
            "     key_words: Neuro-sama、Live2D模型 、VTube Studio\n",
            "========================================\n",
            "76 根據提供的資訊，我們可以回答以下問題：  1. Neuro-sama 的 Live2D 模型最初是使用 VTube Studio 預設角色。  答案：正確。Neuro sama 原本是在 Momose Hiyori 這個預先設定好的模型上運行，後來才有了自己的獨立的第二代和第三版Live ２Ｄモデル  希望這些資訊能夠幫助你！\n",
            "========================================\n",
            "     question: 「Re：從零開始的異世界生活 第三季」動畫中，劫持愛蜜莉雅並想取其為妻的人是誰？\n",
            "\n",
            "     core_question: 核心問題是：在「從零開始的異世界生活 第三季」動畫中，劫持愛蜜莉雅的人身份。\n",
            "     key_words: 愛蜜莉雅、劫持者\n",
            "========================================\n",
            "77 劫持愛蜜莉雅的人身份是「獵腸者」艾爾莎。\n",
            "========================================\n",
            "     question: 《海綿寶寶》的主角海綿寶寶在第五季《失蹤記》中，在哪個城市擊敗刺破泡泡紅眼幫？\n",
            "\n",
            "     core_question: 核心問題是：《海綿寶宝》第五季中，哪個城市的刺破泡沫紅眼幇被擊敗？\n",
            "     key_words: 失蹤記、海綿寶宝\n",
            "========================================\n",
            "78 根據相關資訊，我發現你問的是《海綿寶宝》第五季中，哪個城市的刺破泡沫紅眼幇被擊敗。  答案是：新加坡。\n",
            "========================================\n",
            "     question: 玉米是單子葉還是雙子葉植物？\n",
            "\n",
            "     core_question: 玉米是一種雙子葉植物。\n",
            "     key_words: 玉米、單子葉植物\n",
            "========================================\n",
            "79 玉米是一種雙子葉植物。\n",
            "========================================\n",
            "     question: 中華民國陸軍，隸屬於國防部陸軍司令部，總兵力約為13.2萬，兵力為三軍之最。請問中華民國陸軍軍歌前六字為何？\n",
            "\n",
            "     core_question: 核心問題是：中華民國陸軍的「」歌曲前六字為何？\n",
            "     key_words: 中華民國陸軍、總兵力約為13.2萬、中 華 民 國 陸 軍Military 歌\n",
            "========================================\n",
            "80 根據提供的資訊，中華民國陸軍「」歌曲前六字為：「忠誠精武」。\n",
            "========================================\n",
            "     question: 台大的理工科系大多規定一些基礎自然科學科目，如普通物理和普通化學，作為必修學分，這似乎已成為「普通」跟「理所當然」的規定。但同時也有一些科系，由於科系的專業幾乎並無這些自然科學知識的用武之地，因此便將這些科目的必修學分要求較為放寬。請問台大電資學院哪個系規定物理、化學以及生物科目可以只擇一修習即可？\n",
            "\n",
            "     core_question: 根據台大電資學院的規定，計算機科學與情報工程系（CSIE）是唯一一個允許選擇修習其中一門自然 科目的課程。\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "台大、電資學院、大學規定、本科系選修課程放寬物理化生物\n",
            "========================================\n",
            "81 根據台大電資學院的規定，計算機科學與情報工程系（CSIE）是唯一一個允許選擇修習其中一門自然 科目的課程。  然而，這個問題沒有提供明確相關數字或具體細節，因此無法直接使用該資料來回答。\n",
            "========================================\n",
            "     question: 在月球的火山地形中，廣大而平坦的玄武岩平原地形被稱為月海、月灣或月湖。憂傷湖（Lacus Doloris）、死湖（Lacus Mortis）、忘湖（Lacus Oblivionis）、恐怖湖（Lacus Timoris）以及愛灣（Sinus Amoris），以上五個地形何者位於不面對地球的月球背面？\n",
            "\n",
            "     core_question: 核心問題是：哪五個月球地形位於不面對地球的背側？\n",
            "     key_words: 憂傷湖（Lacus Doloris）、死lake （不確定是否為 Lacus Morti s 或是Mare Imbrium 的部分），忘 lake  ，恐怖Lake，愛灣\n",
            "========================================\n",
            "82 根據提供的資訊，我們可以知道月球地形中不面對地球背側的一些特徵。然而，問題是問哪五個 月 球 地 形 位 於 不 面 對 大 陸 的 背 側。  雖然悲湖（Lacus Doloris）是一種位於雪陸區的月海，但它並不是一個獨立的地形單位，而是在一組地理特徵中的一部分。根據提供資訊，周圍有怨恨 湖、冬 hồ 和柔 Hồ 等 月 球 地 形。  因此，我們需要進一步查找其他不面對地球背側的月球表面的五個主要的地形單位：  1.  **海摩斯山脈**：位於東北方，與悲湖相鄰。 2\\.   \\*\\*怨恨 湖\\_\\_（Lacus Iridum）：西邊緊臨著不面向地球的背側月球表面的地形單位之一。  3.  **冬 hồ**: 位於南部，不直接對應到一個特定的位置，但是在悲湖周圍。 4\\.   \\*\\*柔 湖\\_\\_（Lacus Felicitas）：西邊緊臨著不面向地球的背側月球表面的地形單位之一。  5.  **曼尼里乌斯陨石坑**：位於南方，直359公里。  這五個 月 球 地 形 位 於 不 面 對 大 陸 的 背 側，是一個相對完整的月球表面地理單位組合。\n",
            "========================================\n",
            "     question: 請問由貝多芬所創作的《C♯小調第14號鋼琴奏鳴曲》，其較為人知的別稱是什麼？\n",
            "\n",
            "     core_question: 核心問題是：《C♯小調第14號鋼琴奏鳴曲》有什麼較為人知的別稱？\n",
            "     key_words: 提取的關鍵詞：貝多芬、C♯小調第14號鋼琴奏鳴曲\n",
            "========================================\n",
            "83 《C♯小調第14號鋼琴奏鳴曲》有什麼較為人知的別稱？  答案：該作品最常見的是被叫做「月光」或是 「Moonlight Sonata」。\n",
            "========================================\n",
            "     question: 阿米斯音樂節是在國際上頗負盛名的音樂展演活動，請問阿米斯音樂節是由哪位歌手所舉辦？\n",
            "\n",
            "     core_question: 阿米斯音樂節的核心問題是：誰舉辦了這個活動？\n",
            "     key_words: 阿米斯音樂節、歌手\n",
            "========================================\n",
            "84 根據提供的資訊，阿米斯音樂節是由舒美恩創辦，以原住民族文化為主體舉行於台東縣都蘭部落。\n",
            "========================================\n",
            "     question: 「Poppy Playtime - Chapter 4」遊戲中出現的黏土人叫甚麼名字？\n",
            "\n",
            "     core_question: 核心問題是：遊戲「Poppy Playtime - Chapter 4」中黏土人的名字。\n",
            "     key_words: Poppy Playtime - Chapter 4、黏土人\n",
            "========================================\n",
            "85 根據提供的資訊，我發現了關於遊戲「Poppy Playtime - Chapter 4」中黏土人的名字。   在這個章節，新角色Doey被提及為一名粘土地品人類形象。  因此，最合適答案是：多伊（Doe）\n",
            "========================================\n",
            "     question: 賓茂部落 Djumulj 的現址是 1951 年遷村後的新聚落，Djumulj 在排灣族語裡是「經常豐收，糧食堆積如山之地」的意思。賓茂村是一塊飛地。在地理位置上，賓茂被太麻里鄉所包圍，但賓茂其實屬於何一行政區劃？\n",
            "\n",
            "     core_question: 核心問題是：賓茂村屬於何一行政區劃？\n",
            "     key_words: 以下是提取的關鍵詞，逗號分隔：\n",
            "\n",
            "賓茂部落、DjumulJ、新聚 落 、排灣族語、、太麻里鄉\n",
            "========================================\n",
            "86 根據提供的資訊，賓茂村位於臺東縣金峰鄉，是一座飛地聚落。\n",
            "========================================\n",
            "     question: 義大利文藝復興時期著名雕塑家米開朗基羅創作的《大衛》雕像，最初是在何處雕刻並展現其雕塑藝術成就？\n",
            "\n",
            "     core_question: 核心問題：米開朗基羅《大衛》雕像最初是在何處創作？\n",
            "     key_words: 義大利、文藝復興時期、中世紀後半葉、大衛雕像\n",
            "========================================\n",
            "87 根據相關資訊，米開朗基羅《大衛》雕像最初是在佛罗伦萨的市政厅旧宫入口創作。\n",
            "========================================\n",
            "     question: 中華民國國軍的軍階中，最高級的將領為特級上將，肩、領章上有五顆星星。除了蔣中正之外，另一位曾短暫晉升特級上將的將領是誰？\n",
            "\n",
            "     core_question: 核心問題：除了蔣中正之外，曾短暫晉升特級上將的另一位軍事領袖是誰？\n",
            "     key_words: 蔣中正、特級上將\n",
            "========================================\n",
            "88 根據提供的資訊，除了蔣中正之外，有一位軍事領袖曾短暫晉升特級上將。這個人是李烈钧，因為審判西安事件的一级 上将张学良，但审 判高层军阶不得低于被告人的 军階，故加 特 级行銜\n",
            "========================================\n",
            "     question: 線上遊戲「英雄聯盟」2012年第二賽季世界大賽的總冠軍是哪一個戰隊？\n",
            "\n",
            "     core_question: 核心問題是：2012年第二賽季英雄聯盟世界大赛的總冠軍戰隊。\n",
            "     key_words: 英雄聯盟、2012年第二賽季世界大赛\n",
            "========================================\n",
            "89 根據相關資訊，2012年第二賽季英雄聯盟世界大赛的總冠軍戰隊是台湾队伍TaipeiAssassins。\n",
            "========================================\n",
            "     question: 在日本麻將中，非莊家一開始的手牌有幾張？\n",
            "\n",
            "     core_question: 核心問題是：在日本麻將中，非莊家一開始的手牌有幾張？\n",
            "     key_words: 提取的關鍵詞：日本麻將、莊家\n",
            "========================================\n",
            "90 根据提供的信息，非莊家一開始的手牌有13張。\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"r12942009\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        # if id !=1 :\n",
        "        #   continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(f'{\"=\"*40}')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "            pass\n",
        "        print(f'{\"=\"*40}')\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "          print(id)\n",
        "          continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(f'{\"=\"*40}')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "        print(f'{\"=\"*40}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "outputs": [],
      "source": [
        "# Combine the results into one file.\n",
        "STUDENT_ID = \"r12942009\"\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Correct"
      ],
      "metadata": {
        "id": "j1ylIsnUYs4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "with open(f'./{STUDENT_ID}.txt', 'r') as answer:\n",
        "    answers = answer.readlines()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[1] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        sols = question.split('、')\n",
        "        print(answers[id-1])\n",
        "        for sol in sols:\n",
        "          if sol in answers[id-1]:\n",
        "            correct += 1\n",
        "            print(id, 'sol->', sol, 'Correct !!!')\n",
        "            break\n",
        "        else:\n",
        "          # print(f'{\"=\"*40}')\n",
        "          print(id, 'sol->', sol, ' Incorrect')\n",
        "        print(f'{\"=\"*40}')\n",
        "print(f'Correct: {correct}/{len(questions)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv_L3YxbYsMq",
        "outputId": "f90bfafd-cb5a-484b-ae00-8b39d29451a9"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "根據提供的資訊，「虎山雄風飛揚」這首歌曲是南投縣光華國小校園背景下的學校之作。\n",
            "\n",
            "1 sol-> 光華國小 Correct !!!\n",
            "========================================\n",
            "根據相關資訊，2025年初NCC規定境外郵購自用產品回台的審查費金額為每案新臺幣750元。\n",
            "\n",
            "2 sol-> 750元 Correct !!!\n",
            "========================================\n",
            "根據提供的資訊，第一代 iPhone 是由史蒂夫·乔布斯發表。\n",
            "\n",
            "3 sol-> 史提夫賈伯斯  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，托福網路測驗 TOEFL iBT 的四個部分分別為：閱讀、聽力、中英文溝通和寫作。考生需要在2.5小時內完成這些項目。  關於達到什麼樣得成績才能申請台灣大學進階英語免修，根據提供的資訊並無明確指出具體分數要求。但是，我們可以參照托福官方網站上的資料。一般而言，大多数台湾大学对 TOEFL iBT 考试成绩有以下基本需求：  * 閱讀：22 分  * 聽力 : 23     中英文溝通:26分以上（口語）     寫作25  這些是大致的要求，但具體情況可能會因為學校和院系而異。因此，我們建議考生查閱各大學入學指南以获取詳細資訊。  最後，請注意，這只是基本需求，並不代表你一定能夠免修進階英語課程。你應該根據自己的實力以及所申報的學校和院系來準備。\n",
            "\n",
            "4 sol-> 72分或以上  Incorrect\n",
            "========================================\n",
            "根據問題和相關資訊，我們可以得出以下結論：  觸地 try 可以得到 7 分。  這是因為在橄欖球比賽中，當一名選手持著足球並將其踢入對方的極陣區域時，就會獲得一個達阵（Try），而每個 Try 都可以得分5點。然而，如果該隊伍選擇進行罰門射擊，那麼他們就有機遇再得到另外1或3 分。  因此，觸地 try 可以取得 7 個積極的結果：   * 得到達阵（Try）並獲得了最初五個分數。 + 如果球队选择进行罚门踢，他们可以获得额外的一点积极结果，即得到了額多一點。\n",
            "\n",
            "5 sol-> 5 Correct !!!\n",
            "========================================\n",
            "根據提供的資訊，ruvuwa'an位於現今台東縣卑南鄉。\n",
            "\n",
            "6 sol-> 臺東縣太麻里鄉  Incorrect\n",
            "========================================\n",
            "根據相關資訊，熊仔的碩班指導教授為李琳山。\n",
            "\n",
            "7 sol-> 李琳山 Correct !!!\n",
            "========================================\n",
            "根据提供的信息，迈克尔·法拉第是发现电磁感应定律的人，他于1831年发表了这一研究结果。\n",
            "\n",
            "8 sol-> Michael Faraday  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，距離國立臺灣史前文化博物館最近的是康樂站。\n",
            "\n",
            "9 sol-> 康樂車站  Incorrect\n",
            "========================================\n",
            "20与30的和是50。\n",
            "\n",
            "10 sol-> 50 Correct !!!\n",
            "========================================\n",
            "根據提供的資訊，盧卡·唐西奇在2025年被交易至洛杉磯湖人隊。\n",
            "\n",
            "11 sol-> 洛杉磯湖人 Correct !!!\n",
            "========================================\n",
            "根據相關資訊，2024年美國總統大選的勝出者是唐納德·特朗普。\n",
            "\n",
            "12 sol-> Donald Trump  Incorrect\n",
            "========================================\n",
            "根据问题和相关信息，Llama-3.2模型的参数量最小的是7B。\n",
            "\n",
            "13 sol-> 1B  Incorrect\n",
            "========================================\n",
            "根據國立臺灣大學學則，停修課程的限制有多嚴格？   依照相關資訊顯示，每個學校都有一些規定，但是東海大学和国台大都是每学期最低学习分数不得少于12个单位（日間班）或9 个單位 （進 修 班、延畢生）。\n",
            "\n",
            "14 sol-> 一門  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，DeepSeek公司是由幻方量化創立的一家人工智慧與大型語言模型企業。其母 公司為中资对冲基金—— 幻 方  。\n",
            "\n",
            "15 sol-> 幻方量化（High-Flyer）  Incorrect\n",
            "========================================\n",
            "根据提供的信息，2024年NBA总冠军队伍是波士顿凯尔特人。\n",
            "\n",
            "16 sol-> 波士頓賽爾提克  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，我們可以得出以下結論：  烃類是一種由碳和氫組成的一般化合物，包括了多個不同的分子结构，如單鏈、環狀等。其中，有兩原子的間距形成三鍵的情況是炔（英语：Alkyne）的特徴。  因此，我們可以得出以下結論：  答案: 炎\n",
            "\n",
            "17 sol-> 炔類  Incorrect\n",
            "========================================\n",
            "根据提供的信息，图灵被誉为“计算机科学之父”，他提出了抽象概念——可計算数（computable numbers），并用它来定义什么是可以通过一台叫做圖靈機(Turing machine) 的设备在有限时间内生成或识别出来。\n",
            "\n",
            "18 sol-> Alan Turing  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，臺灣玄天上帝信仰進香中心位於哪個行政區劃內？答案是松柏嶺受鎮宮。\n",
            "\n",
            "19 sol-> 南投縣名間鄉  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，Windows作業系統是由微軟公司開發和製造。\n",
            "\n",
            "20 sol-> 微軟 Correct !!!\n",
            "========================================\n",
            "根據提供的資訊，官將首發源自臺灣新北市新的莊區，是一種傳統民俗陣頭。它由增損兩位将軍擔任護駕、開道，在地藏王菩薩及大眾爺出巡時負責驅除邪煞壯盛威儀。  官將首的步法和阵型较易训练，因此在臺灣陣頭文化中掀起一股風潮，帶動各地方學習仿效。目前官方網站上並未提供相關資訊，但根據其他資料來源顯示，其出場獨有的「喊班」儀式流程嚴謹，有特殊意義。  數位時代下，以新莊地藏庵頭前庄官將首為例，運用動作捕捉技術採集展演的三維資訊，並透過虛擬實境（VR）與增強現实技术展示成果。\n",
            "\n",
            "21 sol-> 新莊地藏庵 Correct !!!\n",
            "========================================\n",
            "很抱歉，但我沒有收到任何相關資訊。若你能提供更多的背景或內容，我將會盡力幫助您找出答案。  如果有關於《咒》的邪神名，請告訴我們哪一部作品中的「_kahal」？\n",
            "\n",
            "22 sol-> 大黑佛母  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，歌詞「短暫交會의旅程就此分岔」出自動力火車（PowerStation）的曲目《路人甲》（Lù rén jiǎ）。\n",
            "\n",
            "23 sol-> 動力火車 Correct !!!\n",
            "========================================\n",
            "根據憲法法院的判決，系爭規定一尚未違反刑罰明確性原則。該條款以「詐術或其他非法律方法」作為犯罪行为之客觀構成要件，其中 「 詮术 」 一詞指使人認知錯誤 之蓄意欺騙手段，受規範對象可依此預見其 行 为的 可罰性。   系爭规定二未违反憲法第23條比例原則及刑罚明确性的要求，也没有构成宪政所不容许之差别待遇，与选举权、平等權和民主正当与公 正 的公共利益相关的意旨均尚无牴觸。\n",
            "\n",
            "24 sol-> Likavung  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，最新輝達顯卡系列號碼為GeForce RTX 50 系列。\n",
            "\n",
            "25 sol-> GeForce RTX 50 系列 Correct !!!\n",
            "========================================\n",
            "根據相關資訊，大S是在日本旅遊時去世。\n",
            "\n",
            "26 sol-> 日本 Correct !!!\n",
            "========================================\n",
            "根據相關資訊，我發現了核心問題是誰发现万有引力。答案是在1687年，艾萨克·牛顿爵士在他的著作《自然哲学的数学原理》中首次提出了萬有的定律。  因此，最终回答如下：  最早的人类科学家之一，是谁发明了“ 万 有 引 力 定 律”？\n",
            "\n",
            "27 sol-> 牛頓  Incorrect\n",
            "========================================\n",
            "根據提供的資訊，台鵠開示計畫「TAIHUCAIS」的英文全名確實為Taiwan Indigenous Health and Welfare Information System。\n",
            "\n",
            "28 sol-> TAIwan HUmanities Conversational AI Knowledge Discovery System  Incorrect\n",
            "========================================\n",
            "答案：《终结者》\n",
            "\n",
            "29 sol-> The Terminator  Incorrect\n",
            "========================================\n",
            "水的化學式為H2O。\n",
            "\n",
            "30 sol-> H2O Correct !!!\n",
            "========================================\n",
            "Correct: 12/30\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}